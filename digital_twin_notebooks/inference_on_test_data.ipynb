{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecff281b",
   "metadata": {},
   "source": [
    "# Inference on test data (2019-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116049f",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64c0e8b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import toml\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.markers as markers\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable    \n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import (LongitudeFormatter, LatitudeFormatter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f571c9",
   "metadata": {},
   "source": [
    "Add to system path the parent folder in order to be able to import the **Fires** library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03acce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "_pth = \"../\"\n",
    "if _pth not in sys.path:\n",
    "\tsys.path.append(_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785d525",
   "metadata": {},
   "source": [
    "Import from **Fires** library what is necessary to build the Inference workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65388acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emanueledonno/VSCode/CMCC/ML4Fires/config\n",
      "/Users/emanueledonno/VSCode/CMCC/ML4Fires/digital_twin_notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emanueledonno/opt/anaconda3/envs/cmcc-torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import Fires\n",
    "from Fires._datasets.torch_dataset import FireDataset\n",
    "\n",
    "from Fires._macros.macros import (\n",
    "    CONFIG,\n",
    "    DATA_DIR,\n",
    "    LOG_DIR,\n",
    "    NEW_DS_PATH,\n",
    "    RUN_DIR,\n",
    "    SCALER_DIR,\n",
    "    TORCH_CFG\n",
    ")\n",
    "\n",
    "from Fires._models.unetpp import UnetPlusPlus as UPP\n",
    "\n",
    "from Fires._scalers.minmax import MinMaxScaler\n",
    "from Fires._scalers.standard import StandardScaler\n",
    "\n",
    "from Fires._utilities.configuration import load_global_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404149b0",
   "metadata": {},
   "source": [
    "## Experiments and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc24fc",
   "metadata": {},
   "source": [
    "### Plot functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18e1c0",
   "metadata": {},
   "source": [
    "Define:\n",
    "- the projection type\n",
    "- the map extension\n",
    "- the arrays with the latitudes and the longitudes that must used for axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define projection\n",
    "datacrs = ccrs.PlateCarree()\n",
    "\n",
    "# define map extent\n",
    "extent_args=dict(extents = [-180, 180, -90, 90], crs=datacrs)\n",
    "\n",
    "# define latitudes and longitudes array that must be used for axes\n",
    "latitudes = np.arange(-60, 90, 30)\n",
    "longitudes = np.arange(-160, 180, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472cdd76",
   "metadata": {},
   "source": [
    "Function used to draw map features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f40085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_features(ax:Any):\n",
    "\t\"\"\"\n",
    "\tThis function adds several geographical features to the map using Cartopy features:\n",
    "\n",
    "    * Political borders: outlines country borders with a solid black line style (':') and a linewidth of 0.5.\n",
    "    * Oceans: outlines the ocean regions with a solid black line style ('-') and a linewidth of 0.8.\n",
    "    * Lakes: outlines lakes with a solid black line style ('-') and a linewidth of 0.8.\n",
    "    * Rivers: outlines rivers with a solid black line style ('-') and a linewidth of 0.8.\n",
    "    * Coastlines: adds high-resolution coastlines (50 meters) to the map with a higher zorder (3) for better visibility.\n",
    "\n",
    "    **Note:** \n",
    "        * Land is not explicitly added in this function. \n",
    "        * Adding a background image using `ax.stock_img()` is not implemented. \n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tax : Any\n",
    "\t\tThe matplotlib axis object to add the features to.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tax : Any\n",
    "\t\tThe modified matplotlib axis object.\n",
    "\n",
    "\t\n",
    "    \n",
    "\t\"\"\"\n",
    "\t# political borders\n",
    "\tax.add_feature(cfeature.BORDERS, linestyle=':',linewidth=0.5, edgecolor='k')\n",
    "\t# add ocean\n",
    "\tax.add_feature(cfeature.OCEAN, linestyle='-',linewidth=0.8, edgecolor='k')\n",
    "\t# add lakes\n",
    "\tax.add_feature(cfeature.LAKES, linestyle='-',linewidth=0.8, edgecolor='k')\n",
    "\t# add rivers\n",
    "\tax.add_feature(cfeature.RIVERS, linestyle='-',linewidth=0.8, edgecolor='k')\n",
    "\t# add land\n",
    "\t# ax.add_feature(cfeature.LAND, zorder=1, edgecolor='k')\n",
    "\t# add coastlines\n",
    "\tax.coastlines(resolution='50m', zorder=3)\n",
    "\t\n",
    "\t# add stock image\n",
    "\t# ax.stock_img()\n",
    "\n",
    "\treturn ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16dae19",
   "metadata": {},
   "source": [
    "Function used to highlight specific burned areas points on map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b63bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_ba(ax:Any, y:float, x:float, color:str):\n",
    "\t\"\"\"\n",
    "\tPlots lines and a circle to highlight a specific point on a map.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tax : Any\n",
    "\t\tThe matplotlib axis object where to plot the elements.\n",
    "\ty : float\n",
    "\t\tThe latitude value of the point to highlight.\n",
    "\tx : float\n",
    "\t\tThe longitude value of the point to highlight.\n",
    "\tcolor : str\n",
    "\t\tThe color to use for the lines and circle.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tax : Any \n",
    "\t\tThe modified matplotlib axis object.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# plot lines corresponding to latitude and longitude value of burned areas\n",
    "\tax.axhline(y=y, color=color, linewidth=3, zorder=3, linestyle=':')\n",
    "\tax.text(-181.5, np.round(y, 2), f'{np.round(y, 2)}', color=color, fontweight='bold', size=50, ha='center', va='center', rotation=90)\n",
    "\t\n",
    "\tax.axvline(x=x, color=color, linewidth=3, zorder=3, linestyle=':')\n",
    "\tax.text(np.round(x, 2), -90.5, f'{np.round(x, 2)}', color=color, fontweight='bold', size=50, ha='center', va='top')\n",
    "\t\n",
    "\t# plot cicle around the pixel with the value of burned areas\n",
    "\tcircle = plt.Circle((x, y), 1, color=color, linewidth=5, fill=False, zorder=3)\n",
    "\tax.add_patch(circle)\n",
    "\t\n",
    "\treturn ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c4974",
   "metadata": {},
   "source": [
    "Funtion to set the axis labels, ticks, and formatters for latitude and longitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e05ebc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_axis(ax, is_y:bool, latlon_vals, gl):\n",
    "\t\"\"\"\n",
    "\tSets the axis labels, ticks, and formatters for latitude or longitude on a map.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tax : Any\n",
    "\t\tThe matplotlib axis object to modify.\n",
    "\tis_y : bool\n",
    "\t\tTrue if setting the y-axis, False for x-axis.\n",
    "\tlatlon_vals : np.array\n",
    "\t\tThe list of latitude or longitude values for the axis.\n",
    "\tgl : Any\n",
    "\t\tThe gridlines object for the map.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tax : Any\n",
    "\t\tThe modified axis object.\n",
    "\t\"\"\"\n",
    "\tvalues = latlon_vals\n",
    "\tif is_y:\n",
    "\t\tlat_formatter = LatitudeFormatter()\n",
    "\t\tax.yaxis.set_major_formatter(lat_formatter)\n",
    "\t\tax.yaxis.set_major_locator(mticker.FixedLocator(values))\n",
    "\t\tax.set_yticklabels(values, fontweight='bold', size=50, rotation=90)\n",
    "\t\tax.set_yticks(values)\n",
    "\t\tgl.xlocator = mticker.FixedLocator(values)\n",
    "\t\tgl.xlines = False\n",
    "\n",
    "\telse:\n",
    "\t\tlon_formatter = LongitudeFormatter(zero_direction_label=True)\n",
    "\t\tax.xaxis.set_major_formatter(lon_formatter)\n",
    "\t\tax.xaxis.set_major_locator(mticker.FixedLocator(values))\n",
    "\t\tax.set_xticklabels(values, fontweight='bold', size=50)\n",
    "\t\tax.set_xticks(values)\n",
    "\t\tgl.ylocator = mticker.FixedLocator(values)\n",
    "\t\tgl.ylines = False\n",
    "\n",
    "\treturn ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994af775",
   "metadata": {},
   "source": [
    "Function to draw Tropic of Cancer, Equator, and Tropic of Capricorn on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcda5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tropics_and_equator(ax):\n",
    "\t\"\"\"\n",
    "\tPlots lines representing the Tropic of Cancer, Equator, and Tropic of Capricorn on a map.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tax : Any\n",
    "\t\tThe matplotlib axis object where to plot the lines.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tax : Any: \n",
    "\t\tThe modified matplotlib axis object.\n",
    "\t\"\"\"\n",
    "\tax.axhline(23.5, linestyle=':', color='blue', linewidth=0.7, label='Tropic of Cancer')\n",
    "\tax.axhline(0.00, linestyle=':', color='black', linewidth=0.7, label='Equator')\n",
    "\tax.axhline(-23.5, linestyle=':', color='blue', linewidth=0.7, label='Tropic of Capricorn')\n",
    "\treturn ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a8858",
   "metadata": {},
   "source": [
    "Function that generates a comprehensive map visualization of the burned areas data, highlighting minimum and maximum values alongside their confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2bc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_map(\n",
    "\tavg_target_data:np.array,\n",
    "\tavg_data_on_lats:np.array,\n",
    "\tlowerbound_data:np.array,\n",
    "\tupperbound_data:np.array,\n",
    "\tlats:list,\n",
    "\tlons:list,\n",
    "\ttitle:str,\n",
    "\tcmap:str) -> None:\n",
    "\t\"\"\"\n",
    "\tGenerates a comprehensive map visualization of a dataset, highlighting \n",
    "\tminimum and maximum values alongside their confidence intervals.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tavg_target_data : np.array\n",
    "\t\t2D array containing the core data to be visualized as color intensity on the map.\n",
    "\t\tMissing values (NaN) are handled by setting the color to transparent.\n",
    "\n",
    "\tavg_data_on_lats : np.array\n",
    "\t\t1D array containing the average of the target data for each latitude value.\n",
    "\t\tThis data is plotted as a line in a secondary subplot.\n",
    "\n",
    "\tlowerbound_data : np.array\n",
    "\t\t2D array containing the lower bound of the data (e.g., standard deviation or confidence interval) for each latitude and longitude.\n",
    "\t\tThis data is used to shade the area around the average line in the secondary subplot.\n",
    "\n",
    "\tupperbound_data : np.array\n",
    "\t\t2D array containing the upper bound of the data for each latitude and longitude.\n",
    "\t\tSimilar to `lowerbound_data`, it's used for shading the confidence interval in the secondary subplot.\n",
    "\n",
    "\tlats : list\n",
    "\t\tList containing the latitude values corresponding to the data.\n",
    "\n",
    "\tlons : list\n",
    "\t\tList containing the longitude values corresponding to the data.\n",
    "\n",
    "\ttitle : str\n",
    "\t\tThe title to be displayed at the top of the plot.\n",
    "\n",
    "\tcmap : str\n",
    "\t\tThe name of the colormap to use for visualizing the data on the map.\n",
    "\t\n",
    "\tReturns\n",
    "\t-------\n",
    "\tNone\n",
    "\t\tSaves the figure as a high-resolution PNG image (300 dpi) but does not return anything.\n",
    "\t\"\"\"\n",
    "\t# define color\n",
    "\tcolor = 'darkred' #fc6742 #4296fc #990e0e\n",
    "\t\n",
    "\t# compute maximum along latitudes and longitudes and find index\n",
    "\tmaximum_val = np.nanmax(avg_target_data)\n",
    "\tlat_idx_max, lon_idx_max  = np.where(avg_target_data==maximum_val)\n",
    "\tmax_val_latitude = lats[lat_idx_max][0]\n",
    "\tmax_val_longitude = lons[lon_idx_max][0]\n",
    "\t\n",
    "\t# compute minimum along latitudes and longitudes and find index\n",
    "\tminimum_val = np.nanmin(avg_target_data)\n",
    "\tlat_idx_min, lon_idx_min  = np.where(avg_target_data==minimum_val)\n",
    "\tmin_val_latitude = lats[lat_idx_min][0]\n",
    "\tmin_val_longitude = lons[lon_idx_min][0]\n",
    "\t\n",
    "\t# define fiure and subplots\n",
    "\t_, ax1 = plt.subplots(figsize=(90, 80), subplot_kw=dict(projection=datacrs), sharey=True)\n",
    "\t\n",
    "\t# set title of the plot\n",
    "\tax1.set_title(title, fontweight='bold', size=80)\n",
    "\t\n",
    "\t# set x and y labels\n",
    "\tax1.set_xlabel('Longitude [deg]', fontweight='bold', size=50)\n",
    "\tax1.set_ylabel('Latitude [deg]', fontweight='bold', size=50)\n",
    "\t\n",
    "\t# set map extent\n",
    "\tax1.set_extent(**extent_args)\n",
    "\t\n",
    "\t# plot map features such as borders, sea, lakes, rivers and background image\n",
    "\tax1 = draw_features(ax=ax1)\n",
    "\t\n",
    "\t# plot data on the map\n",
    "\tcmap = plt.get_cmap(cmap)\n",
    "\tcmap.set_under((0, 0, 0, 0))\n",
    "\th = ax1.pcolormesh(lons, lats, avg_target_data, transform=datacrs, cmap=cmap, zorder=3, alpha=0.5)\n",
    "\t\n",
    "\t# highlight pixel where the maximum value ov burned areas has been found and put a circle around it\n",
    "\tax1 = highlight_ba(ax=ax1, y=max_val_latitude, x=max_val_longitude, color=color)\n",
    "\t\n",
    "\t# highlight pixel where the minimum value ov burned areas has been found and put a circle around it\n",
    "\tax1 = highlight_ba(ax=ax1, y=min_val_latitude, x=min_val_longitude, color='green')\n",
    "\t\n",
    "\t# add grid lines for latitude and longitude\n",
    "\tgl = ax1.gridlines(crs=datacrs, draw_labels=False, linewidth=1.5, color='gray', alpha=0.5, linestyle='-', zorder=3)\n",
    "\t\n",
    "\t# define longitudes and set x ticks\n",
    "\tax1 = set_axis(ax=ax1, is_y=False, latlon_vals=longitudes, gl=gl)\n",
    "\t\n",
    "\t# define latitudes and set y ticks\n",
    "\tax1 = set_axis(ax=ax1, is_y=True, latlon_vals=latitudes, gl=gl)\n",
    "\t\n",
    "\t# define latitudes for tropics and equator\n",
    "\tax1 = draw_tropics_and_equator(ax=ax1)\n",
    "\t\n",
    "\t# add subplot\n",
    "\tdivider = make_axes_locatable(ax1)\n",
    "\tax2 = divider.append_axes(\"right\", size=\"10%\", pad=0.5, axes_class=plt.Axes)\n",
    "\t\n",
    "\t# plot data\n",
    "\tax2.plot(avg_data_on_lats, lats, color='red', linewidth=1)\n",
    "\tax2.plot(upperbound_data, lats, alpha=0.3, color='black', linewidth=0.5)\n",
    "\tax2.plot(lowerbound_data, lats, alpha=0.3, color='black', linewidth=0.5)\n",
    "\t\n",
    "\t# fill space between lines\n",
    "\tax2.fill_betweenx(y=lats, x1=avg_data_on_lats, x2=upperbound_data, color='gray', alpha=0.15)\n",
    "\tax2.fill_betweenx(y=lats, x1=avg_data_on_lats, x2=lowerbound_data, color='gray', alpha=0.15)\n",
    "\t\n",
    "\t# define latitudes for tropics (in degrees) and equator\n",
    "\tax2 = draw_tropics_and_equator(ax=ax2)\n",
    "\t\n",
    "\t# plot max position\n",
    "\tax2.axhline(max_val_latitude, color=color, linewidth=3)\n",
    "\t\n",
    "\t# plot min position\n",
    "\tax2.axhline(min_val_latitude, color='green', linewidth=3)\n",
    "\t\n",
    "\t# set x label\t\n",
    "\tax2.set_xlabel(' Mean ', fontweight='bold', size=50, labelpad=50)\n",
    "\t\n",
    "\t# create list of max values\n",
    "\tax2_vals = np.around([np.nanmin(lowerbound_data, axis=0), np.nanmax(avg_data_on_lats, axis=0), np.nanmax(upperbound_data, axis=0)], 2)\n",
    "\t\n",
    "\t# plot axes tick lines§\n",
    "\tfor tick in ax2_vals:\n",
    "\t\tax2.axvline(x=tick, color='blue', alpha=1, linewidth=1, linestyle=':')\n",
    "\t\tax2.text(round(tick), -.005, f'{round(tick)}', color='blue', fontweight='bold', size=50, transform=ax2.get_xaxis_transform(), ha='center', va='top')\n",
    "\n",
    "\tax2.text(0, -.005, '0', color='black', fontweight='bold', size=50, transform=ax2.get_xaxis_transform(), ha='center', va='top') \n",
    "\t\n",
    "\tax2.set_xticks([])\n",
    "\tax2.set_yticks([])\n",
    "\tax2.set_ylim(bottom=-90, top=90)\n",
    "\tax2.margins(y=0)\n",
    "\t# ax2.autoscale_view(scaley=True)\n",
    "\t\t\n",
    "\t# add colorbar plot\n",
    "\tax_cb = divider.append_axes(\"right\", size=\"2%\", pad=0.3, axes_class=plt.Axes)\n",
    "\tcbar = plt.colorbar(h, ax_cb)\n",
    "\tcbar.ax.tick_params(labelsize=30)\n",
    "\tcbar.ax.set_ylabel('Hectares', color='black', fontweight='bold', size=50, labelpad=50, rotation=270)\n",
    "\t\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"./img/fcci {title}.png\", dpi=300)\n",
    "\t# plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d7ece",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e57247",
   "metadata": {},
   "source": [
    "The first step in the inference pipeline is to select the neural network configuration (**experiment**) and the trained model with that configuration, to generate the burned area maps with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed97bbc5-f63f-48ad-a260-015c8622a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment path: /Users/emanueledonno/VSCode/CMCC/ML4Fires/experiments/20240311_222733\n"
     ]
    }
   ],
   "source": [
    "# define list of folders with the experiments\n",
    "experiment_paths = ['20240311_222733']\n",
    "\n",
    "# define main path to experiments folder\n",
    "main_path = os.path.join(os.path.dirname(os.getcwd()), \"experiments\")\n",
    "\n",
    "# define single experiment path\n",
    "single_path = os.path.join(main_path, str(experiment_paths[0]))\n",
    "print(f\"Experiment path: {single_path}\")\n",
    "\n",
    "# define a common dictionary with all the experiments\n",
    "exp_dicts = dict()\n",
    "for folder in experiment_paths:\n",
    "\tpath = os.path.join(main_path, folder)\n",
    "\tfor file in os.listdir(path):\n",
    "\t\tif file.endswith(\".toml\"):\n",
    "\t\t\texp_args = toml.load(os.path.join(path, file))\n",
    "\t\t\texp_dicts[file.split('.')[0]] = exp_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "124fb02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['exp_5'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dicts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50750267",
   "metadata": {},
   "source": [
    "This notebook showcases an example of using a preliminary version of the Digital Twin. In this first part, we will focus on creating the reference dataset for inference, which is a prerequisite to running the subsequent cells of the notebook. Steps:\n",
    "\n",
    "1. **Download the SeasFireCube v3 dataset**:\n",
    "\tDownload the [SeasFireCube v3](https://zenodo.org/records/8055879) dataset and save it in the `data` folder.\n",
    "\t\n",
    "2. **Load the dataset with `xarray`**\n",
    "\tImport the xarray library and load the SeasFireCube v3 dataset saved in the `data` folder.\n",
    "\n",
    "3. **Select variables**\n",
    "\tUse the experiment configuration `/experiments/20240311_222733/exp_5.toml` to identify the `drivers` and `targets` variables to be used in the model. Define these variables and select related data from previously loaded dataset.\n",
    "\n",
    "4. **Expand the `lsm` variable**\n",
    "\tThe `lsm` variable represents the land-sea mask. Expand this variable to cover the entire time range of the dataset.\n",
    "\n",
    "5. **Select the years of interest**\n",
    "\tSelect the years of interest for the analysis, in this case 2019 and 2020. Filter the dataset based on these years.\n",
    "\n",
    "6. **Save the reference dataset**\n",
    "\tSave the filtered and prepared reference dataset to the data folder.\n",
    "\n",
    "7. **Update the path in the configuration file**\n",
    "\tOpen the experiment configuration file `/experiments/20240311_222733/exp_5.toml`. Update the value of the `path_to_zarr` key with the full path to the saved reference dataset in the data folder.\n",
    "\n",
    "> [!NOTE]\n",
    "> The SeasFireCube v3 dataset is large in size and may not be possible to upload directly to GitHub.\n",
    "> Creating the reference dataset is a preliminary step required to run the subsequent cells of the notebook.\n",
    "> Make sure you have downloaded and saved the SeasFireCube v3 dataset correctly to the data folder before proceeding.\n",
    "> Verify that the path to the reference dataset is updated correctly in the experiment configuration file.\n",
    "\n",
    "\n",
    "After successfully completing the steps described in this section, you will have created the reference dataset necessary to run the subsequent cells of the notebook and test the Digital Twin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2d739",
   "metadata": {},
   "source": [
    "The Python code in the cell below iterates through experiments in a dictionary named `exp_dicts` and performs the following tasks for each experiment:\n",
    "* Loads the experiment configuration.\n",
    "* Loads a pre-trained model and its optimizer and scheduler.\n",
    "* Defines a scaler to normalize the test data.\n",
    "* Creates a torch dataset and data loader for the test data.\n",
    "* Predicts burned areas using the loaded model.\n",
    "* Loads real burned area data.\n",
    "* Calculates statistics (mean, standard deviation) for both predicted and real burned areas.\n",
    "* Calculates the difference between predicted and real burned areas.\n",
    "* Plots the result maps for predicted, real, and difference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981919c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in exp_dicts.keys():\n",
    "\t\n",
    "\t# define current experiment\n",
    "\t# key = list(exp_dict.keys())[0]\n",
    "\tcurr_exp = exp_dicts[key]\n",
    "\tcurr_exp.keys()\n",
    "\t\n",
    "\t#\n",
    "\t#\t\t\t\t\t\t\t\t\t\tDefine model\n",
    "\t#\n",
    "\n",
    "\t# define dictionary containing all model related features\n",
    "\tmodel_dict = curr_exp['model']\n",
    "\t# get model path\n",
    "\tlast_model_path = model_dict['last_model'].split(\"wildfires/\")[1]\n",
    "\t# load model from path\n",
    "\tlast_model_dict = torch.load(last_model_path, map_location=torch.device('cpu'))\n",
    "\t# get model state\n",
    "\tlm_state = last_model_dict['model']\n",
    "\t# get model optimizer\n",
    "\tlm_optimizer = last_model_dict['optimizer']\n",
    "\t# get model scheduler\n",
    "\tlm_scheduler = last_model_dict['scheduler']\n",
    "\n",
    "\t# get model class\n",
    "\tmdl_cls = eval(model_dict['cls'])\n",
    "\t# get model arguments\n",
    "\tmdl_arg = model_dict['args']\n",
    "\t# create a dictionary with all model arguments\n",
    "\targs_ = dict()\n",
    "\t# define model arguments\n",
    "\tfor k in mdl_arg.keys():\n",
    "\t\targs_[k] = eval(mdl_arg[k]) if k == \"activation\" else mdl_arg[k]\n",
    "\t# define mdel\n",
    "\tmodel = mdl_cls(**args_)\n",
    "\t\n",
    "\t# load weights\n",
    "\tmodel.load_state_dict(lm_state)\n",
    "\t# evaluate model\n",
    "\tmodel.eval()\n",
    "\n",
    "\t#\n",
    "\t#\t\t\t\t\t\t\t\t\tDataset for testing model\n",
    "\t#\n",
    "\n",
    "\t# define path to complete dataset\n",
    "\tdataset_path = \"./data/seasfire_v03.zarr\"\n",
    "\t\n",
    "\t# get land sea mask and substitute zeros with NaN values\n",
    "\tlsm = xr.open_zarr(dataset_path)['lsm'].values\n",
    "\tlsm[lsm == 0] = np.nan\n",
    "\t\n",
    "\t# get valid mask data and retrieve valid dates for test data\n",
    "\tvalid_mask_data = xr.open_zarr(dataset_path)['fcci_ba_valid_mask'].sel(time=slice('2019', '2021'))\n",
    "\tvalid_dates = [time for time in valid_mask_data.time.data if valid_mask_data.sel(time=str(time)) == 1]\n",
    "\t\n",
    "\t# define main path, main dataset and select valid dates\n",
    "\tmain_path = \"./data/sfv03_fcci.zarr\"\n",
    "\tmain_data = xr.open_zarr(main_path)\n",
    "\tmain_data = main_data.sel(time = slice(str(valid_dates[0]), str(valid_dates[-1])))\n",
    "\t\n",
    "\t# get drivers and targets\n",
    "\tdrivers = curr_exp['features']['drivers']\n",
    "\ttargets = curr_exp['features']['targets']\n",
    "\t\n",
    "\t# define test datasets for drivers and targets\n",
    "\tdriver_ds = main_data[drivers].load()\n",
    "\ttarget_ds = main_data[targets].load()\n",
    "\t\n",
    "\t# get training years list and define training data\n",
    "\ttrn_years = curr_exp['dataset']['trn_years']\n",
    "\ttrn_data = main_data[drivers].sel(time=slice(str(trn_years[0]), str(trn_years[-1])))\n",
    "\t\n",
    "\t#\n",
    "\t# \t\t\t\tLoad mean and standard deviation maps and create scaler\n",
    "\t#\n",
    "\n",
    "\t# load mean and standard deviation point dataset in order to scale test data\n",
    "\tmean_ds = xr.load_dataset(curr_exp['scalers']['paths']['fcci_mean_point_map']).load()\n",
    "\tstdv_ds = xr.load_dataset(curr_exp['scalers']['paths']['fcci_stdv_point_map']).load()\n",
    "\t\n",
    "\t# define scaler for drivers\n",
    "\tscaler_cls = eval(curr_exp['scalers']['cls'].split(\"'\")[1])\n",
    "\tx_scaler = scaler_cls(mean_ds=mean_ds, stdv_ds=stdv_ds, features=drivers)\n",
    "\t\n",
    "\t#\n",
    "\t#\t\t\t\t\t\tCreate torch dataset and data loader\n",
    "\t#\n",
    "\t\n",
    "\t# define FireDataset arguments and FireDataset torch dataset for test data\n",
    "\tfire_ds_args = dict(src=main_path, drivers=drivers, targets=targets)\n",
    "\ttest_torch_ds = FireDataset(**fire_ds_args, years=list(range(2019, 2022)), scalers=[x_scaler, None])\n",
    "\t\n",
    "\t# define test data loader\n",
    "\ttest_dl_args = dict(batch_size=1, shuffle=True, drop_last=curr_exp['trainer']['drop_reminder'])\n",
    "\ttest_loader = torch.utils.data.DataLoader(test_torch_ds, **test_dl_args)\n",
    "\n",
    "\t\n",
    "\t#\n",
    "\t#\t\t\t\t\t\t\t\tPredict burned areas\n",
    "\t#\n",
    "\t\n",
    "\t# define predictions list, predict data and store them\n",
    "\tpreds = []\n",
    "\twith torch.no_grad():\n",
    "\t\ti = 0\n",
    "\t\tfor data, _ in tqdm(test_loader):\n",
    "\t\t\tif i >= 92: break\n",
    "\t\t\tpreds.append(model(data))\n",
    "\t\t\ti += 1\n",
    "\t\n",
    "\t# define predictions array as vertical stack of predictions list\n",
    "\tpreds_arr = np.vstack(preds)\n",
    "\t\n",
    "\t# ------------------------------------------------------------------------------------\n",
    "\t\n",
    "\t# define max hectares value\n",
    "\tmax_hectares = pow((111/4), 2) * 100\n",
    "\t\n",
    "\t# define latitudes and longitudes lists\n",
    "\tlats = target_ds.latitude.data\t\n",
    "\tlons = target_ds.longitude.data\n",
    "\tprint(lats[:5])\n",
    "\tprint(lons[:5])\n",
    "\t\n",
    "\t# ------------------------------------------------------------------------------------\n",
    "\t\n",
    "\t# real target data (temporal mean)\n",
    "\tavg_real_on_time = target_ds.fcci_ba.mean(dim='time', skipna=True).data\n",
    "\t\n",
    "\t# mask mean dataset with the land sea mask and rescale to original size\n",
    "\tavg_real_descaled = avg_real_on_time * lsm * max_hectares\n",
    "\t\n",
    "\t# compute the average along latitudes and longitudes\n",
    "\tavg_real_on_lats = np.nanmean(avg_real_descaled, axis=1)\n",
    "\tavg_real_on_lons = np.nanmean(avg_real_descaled, axis=0)\n",
    "\t\n",
    "\t# real target data (temporal standard deviation)\n",
    "\tstd_real_on_time = target_ds.fcci_ba.std(dim='time', skipna=True).data\n",
    "\t\n",
    "\t# mask standard deviation data with the land sea mask and rescale to original size\n",
    "\tstd_real_descaled = std_real_on_time * lsm * max_hectares\n",
    "\t\n",
    "\t# compute the average along latitudes and longitudes\n",
    "\tstd_real_on_lats = np.nanmean(std_real_descaled, axis=1)\n",
    "\tstd_real_on_lons = np.nanmean(std_real_descaled, axis=0)\n",
    "\t\n",
    "\t# define upperbound and lowerbound data for plotting the average on latitudes\n",
    "\treal_upperbound = avg_real_on_lats + std_real_on_lats\n",
    "\treal_lowerbound = avg_real_on_lats - std_real_on_lats\n",
    "\t\n",
    "\tprint(f\" Mean real {avg_real_descaled.shape}\")\n",
    "\tprint(f\" Max: {np.nanmax(avg_real_descaled)}\")\n",
    "\tprint(f\" Min: {np.nanmin(avg_real_descaled)}\")\n",
    "\tprint(f\" Lats: {np.nanmax(avg_real_on_lats)}\")\n",
    "\tprint(f\" Lons: {np.nanmax(avg_real_on_lons)}\\n\")\n",
    "\t\n",
    "\tprint(f\" Stdv real {std_real_descaled.shape}\")\n",
    "\tprint(f\" Max: {np.nanmax(std_real_descaled)}\")\n",
    "\tprint(f\" Min: {np.nanmin(std_real_descaled)}\")\n",
    "\tprint(f\" Lats: {np.nanmax(std_real_on_lats)}\")\n",
    "\tprint(f\" Lons: {np.nanmax(std_real_on_lons)}\\n\")\n",
    "\t\n",
    "\tprint(f\"Upperbound: {np.nanmin(real_upperbound)} \\t {np.nanmax(real_upperbound)}\\n\")\n",
    "\tprint(f\"Lowerbound: {np.nanmin(real_lowerbound)} \\t {np.nanmax(real_lowerbound)}\\n\")\n",
    "\t\n",
    "\t__x_idx, __y_idx = np.where(avg_real_on_time==np.nanmax(avg_real_on_time))\n",
    "\tprint(lats[__x_idx], lons[__y_idx])\n",
    "\t\n",
    "\t\n",
    "\t# ------------------------------------------------------------------------------------\n",
    "\t\n",
    "\t# compute temporal mean for prediciton\n",
    "\tavg_preds_on_time = np.nanmean(preds_arr, axis=0)[0, ...]\n",
    "\t\n",
    "\t# mask mean dataset with the land sea mask and rescale to original size\n",
    "\tavg_preds_descaled =  avg_preds_on_time * lsm * max_hectares\n",
    "\t\n",
    "\t# compute the average along latitudes and longitudes\n",
    "\tavg_preds_on_lats = np.nanmean(avg_preds_descaled, axis=1)\n",
    "\tavg_preds_on_lons = np.nanmean(avg_preds_descaled, axis=0)\n",
    "\t\n",
    "\t# compute temporal standard deviation for prediction\n",
    "\tstd_preds_on_time = np.nanstd(preds_arr, axis=0)[0, ...]\n",
    "\t\n",
    "\t# mask standard deviation data with the land sea mask and rescale to original size\n",
    "\tstd_preds_descaled = std_preds_on_time * lsm * max_hectares\n",
    "\t\n",
    "\t# compute the average along latitudes and longitudes\n",
    "\tstd_preds_on_lats = np.nanmean(std_preds_descaled, axis=1)\n",
    "\tstd_preds_on_lons = np.nanmean(std_preds_descaled, axis=0)\n",
    "\t\n",
    "\t# define upperbound and lowerbound data for plotting the average on latitudes\n",
    "\tpreds_upperbound = avg_preds_on_lats + std_preds_on_lats\n",
    "\tpreds_lowerbound = avg_preds_on_lats - std_preds_on_lats\n",
    "\t\n",
    "\tprint(f\"{preds_arr.shape} \\n {len(preds)} \\n {len(valid_dates)} \\n \")\n",
    "\t\n",
    "\tprint(f\" Mean predictions {avg_preds_descaled.shape}\")\n",
    "\tprint(f\" Max: {np.nanmax(avg_preds_descaled)}\")\n",
    "\tprint(f\" Min: {np.nanmin(avg_preds_descaled)}\")\n",
    "\tprint(f\" Lats: {np.nanmax(avg_preds_on_lats)}\")\n",
    "\tprint(f\" Lons: {np.nanmax(avg_preds_on_lons)}\\n\")\n",
    "\t\n",
    "\tprint(f\" Stdv predictions {std_preds_descaled.shape}\")\n",
    "\tprint(f\" Max: {np.nanmax(std_preds_descaled)}\")\n",
    "\tprint(f\" Min: {np.nanmin(std_preds_descaled)}\")\n",
    "\tprint(f\" Lats: {np.nanmax(std_preds_on_lats)}\")\n",
    "\tprint(f\" Lons: {np.nanmax(std_preds_on_lons)}\\n\")\n",
    "\t\n",
    "\tprint(f\"Upperbound: {np.nanmin(preds_upperbound)} \\t {np.nanmax(preds_upperbound)}\\n\")\n",
    "\tprint(f\"Lowerbound: {np.nanmin(preds_lowerbound)} \\t {np.nanmax(preds_lowerbound)}\\n\")\n",
    "\t\n",
    "\tprint(np.where(avg_preds_on_time==np.nanmax(avg_preds_on_time)))\n",
    "\t__x_idx, __y_idx = np.where(avg_preds_on_time==np.nanmax(avg_preds_on_time))\n",
    "\tprint(lats[__x_idx], lons[__y_idx])\n",
    "\t\n",
    "\t# ------------------------------------------------------------------------------------\n",
    "\t\n",
    "\t# compute the difference between real and predicted data\n",
    "\tavg_difference = (avg_real_descaled - avg_preds_descaled)\n",
    "\tavg_diff_on_lats = np.nanmean(avg_difference, axis=1)\n",
    "\tavg_diff_on_lons = np.nanmean(avg_difference, axis=0)\n",
    "\t\n",
    "\tstd_difference = (std_real_descaled - std_preds_descaled)\n",
    "\tstd_diff_on_lats = np.nanmean(std_difference, axis=1)\n",
    "\tstd_diff_on_lons = np.nanmean(std_difference, axis=0)\n",
    "\t\n",
    "\t# define upperbound and lowerbound data for plotting the average on latitudes\n",
    "\tdiff_upperbound = avg_diff_on_lats + std_diff_on_lats\n",
    "\tdiff_lowerbound = avg_diff_on_lats - std_diff_on_lats\n",
    "\t\n",
    "\tprint(f\" Mean difference {avg_difference.shape}\")\n",
    "\tprint(f\" Max: {np.nanmax(avg_difference)}\")\n",
    "\tprint(f\" Min: {np.nanmin(avg_difference)}\")\n",
    "\tprint(f\" Lats: {np.nanmax(avg_diff_on_lats)}\")\n",
    "\tprint(f\" Lons: {np.nanmax(avg_diff_on_lons)}\\n\")\n",
    "\t\n",
    "\tprint(f\" Stdv difference {std_difference.shape}\")\n",
    "\tprint(f\" Max: {np.nanmax(std_difference)}\")\n",
    "\tprint(f\" Min: {np.nanmin(std_difference)}\")\n",
    "\tprint(f\" Lats: {np.nanmax(std_diff_on_lats)}\")\n",
    "\tprint(f\" Lons: {np.nanmax(std_diff_on_lons)}\\n\")\n",
    "\t\n",
    "\tprint(f\"Upperbound: {np.nanmin(diff_upperbound)} \\t {np.nanmax(diff_upperbound)}\\n\")\n",
    "\tprint(f\"Lowerbound: {np.nanmin(diff_lowerbound)} \\t {np.nanmax(diff_lowerbound)}\\n\")\n",
    "\t\n",
    "\tprint(np.where(avg_difference==np.nanmax(avg_difference)))\n",
    "\t__x_idx_max, __y_idx_max= np.where(avg_difference==np.nanmax(avg_difference))\n",
    "\tprint(lats[__x_idx_max], lons[__y_idx_max])\n",
    "\t\n",
    "\t# ------------------------------------------------------------------------------------\n",
    "\t\n",
    "\texp_name = curr_exp['exp_name']\n",
    "\t\n",
    "\tplot_dataset_map(\n",
    "\t\tavg_target_data=avg_preds_descaled,\n",
    "\t\tavg_data_on_lats=avg_preds_on_lats,\n",
    "\t\tlowerbound_data=preds_lowerbound,\n",
    "\t\tupperbound_data=preds_upperbound,\n",
    "\t\tlats=lats,\n",
    "\t\tlons=lons,\n",
    "\t\ttitle = f'Predicted Burned Areas', # FCCI Burned Areas - {exp_name.upper()} Predictions',\n",
    "\t\tcmap = 'nipy_spectral_r') # 'CMRmap'\n",
    "\t\n",
    "\t\n",
    "\tplot_dataset_map(\n",
    "\t\tavg_target_data=avg_real_descaled,\n",
    "\t\tavg_data_on_lats=avg_real_on_lats,\n",
    "\t\tlowerbound_data=real_lowerbound,\n",
    "\t\tupperbound_data=real_upperbound,\n",
    "\t\tlats=lats,\n",
    "\t\tlons=lons,\n",
    "\t\ttitle = f'FCCI Burned Areas - {exp_name.upper()} Real',\n",
    "\t\tcmap = 'nipy_spectral_r') # 'CMRmap'\n",
    "\t\n",
    "\t\n",
    "\tplot_dataset_map(\n",
    "\t\tavg_target_data=avg_difference,\n",
    "\t\tavg_data_on_lats=avg_diff_on_lats,\n",
    "\t\tlowerbound_data=diff_lowerbound,\n",
    "\t\tupperbound_data=diff_upperbound,\n",
    "\t\tlats=lats,\n",
    "\t\tlons=lons,\n",
    "\t\ttitle = f'FCCI Burned Areas - {exp_name.upper()} Difference',\n",
    "\t\tcmap = 'gist_rainbow_r') # 'CMRmap'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmcc-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
